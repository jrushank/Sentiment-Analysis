{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      text  Target\n",
      "218153                     no 3g wireless in farm country        0\n",
      "865260                    thank you for your warm welcome        4\n",
      "526545   twitterfox is addictive CASS you did not quote...       0\n",
      "351397   i hate thunder x 156873582395823678 ugh whyyyy...       0\n",
      "641337   Actual last day of college Pity it has to fini...       0\n",
      "327040                               omgg thts so luckyyy        0\n",
      "180515                          Is not feeling well again        0\n",
      "675273   going to summer school on cruches and my compu...       0\n",
      "500353                                           is tired        0\n",
      "871672   going to my orientation and then baby sitting ...       4\n",
      "959683   lol that helps too but i have a sexy mcfly mug xx       4\n",
      "989230   its near twister everone is moving but the ren...       4\n",
      "1037868                  hello thank you for following me        4\n",
      "491529   these people are driving me crazy Im a human b...       0\n",
      "444810   Why isn t there a drink that can knock me in a...       0\n",
      "759146   http twitpic com 86z9u me and my cousin gavin ...       0\n",
      "450296   hey I thought I was the cantankerous one not f...       0\n",
      "938878   there is thin line between monopoly and busine...       4\n",
      "754388   i don t want jon and kate to get a divorce i h...       0\n",
      "224503   I knew the Cavs were gonna lose going into the...       0\n",
      "188892                                 Not tired at allll        0\n",
      "852532   ok I gotta go back to getting my ass pwned on ...       4\n",
      "568650   I m coming in at 2 That s probably when you re...       0\n",
      "468403   ready for work got to love radio only on the net        0\n",
      "246766   i love sundays headin to pool with spendin som...       0\n",
      "501331   Ugh I hate having tuitions in the morning Its ...       0\n",
      "888641         What Why don t you give the shirt to a fan        4\n",
      "964333   email our team at We_Can_Help cable comcast co...       4\n",
      "758546   I lost by one year while playing quot remember...       0\n",
      "259643   Cant sleep want ice cream have to go back to c...       0\n",
      "...                                                    ...     ...\n",
      "500529       Aweee hell You musta done a job on your knee        0\n",
      "721967   Checked the USGS and we didn t have an earthqu...       0\n",
      "146341    haha i am soo glad you didn t say bananas man...       0\n",
      "966056   congratulations and all the best for you and H...       4\n",
      "845048    which is just what T rex was thinking 200 mil...       4\n",
      "850995   happy birthday to me gettin ready for work and...       4\n",
      "192343   im in the room with my sis and being bored and...       0\n",
      "726578   I hope so I just talked with Neal I can only h...       0\n",
      "417618   if I get the hiccups I ll seriously have them ...       0\n",
      "928729        Going out for a bite to EAT Back in a FLASH        4\n",
      "849225   is having dinner at cafe cartel marina to cele...       4\n",
      "843344   April 20 quot has long been an unofficial day ...       4\n",
      "1038401  MEAN MAN SO MEAN JK Have to pick on U alittle ...       4\n",
      "44268                           I think he is ignoring me        0\n",
      "571446                         That s not very reassuring        0\n",
      "972391   thx I know the over abundance can be annoying ...       4\n",
      "532988   i m going to college after this summer and i l...       0\n",
      "528885   Now what No more Lakers BBQ Parties The World ...       0\n",
      "82954    Gah Comp is acting up Dreadfully this means my...       0\n",
      "584982   Thank you I hate that it looks different on di...       0\n",
      "811608   today s renewed wedding invitation http www et...       4\n",
      "658680   It is I just worry that the computer will cras...       0\n",
      "131267   The seniors class song is We All Roll Along an...       0\n",
      "542163   Thanks for inviting me I thought her birthday ...       0\n",
      "877443                                     loves shopping        4\n",
      "881862   just woken up feel amazing that i havent got w...       4\n",
      "624014   Going batty but of course I m a nut Breaker ke...       0\n",
      "207623   doesn t want her caroline edwards original tat...       0\n",
      "376763             1 2 the frog nicely attatched i miss S        0\n",
      "240527   I m in the doghouse as i chucked OUR pool in t...       0\n",
      "\n",
      "[1500 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "#in the dataset i added titles to all the different columns\n",
    "import pandas as pd \n",
    "import re\n",
    "\n",
    "df = pd.read_csv(\"training_data.csv\", encoding='latin-1')\n",
    "\n",
    "df2 = df.sample(n = 1500, random_state = 30)\n",
    "df3 = df2[['text','Target']]\n",
    "\n",
    "p1 = re.compile(r\"@\\w+\\s\")\n",
    "p2 = re.compile(r\"\\W+\")\n",
    "tweet = df3['text']\n",
    "tweet = tweet.replace({p1: \"\"}, regex=True)\n",
    "tweet = tweet.replace({p2: \" \"}, regex=True)\n",
    "df3['text'] = tweet\n",
    "\n",
    "print(df3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      text  Target\n",
      "218153                     no 3g wireless in farm country       -1\n",
      "865260                    thank you for your warm welcome        1\n",
      "526545   twitterfox is addictive CASS you did not quote...      -1\n",
      "351397   i hate thunder x 156873582395823678 ugh whyyyy...      -1\n",
      "641337   Actual last day of college Pity it has to fini...      -1\n",
      "327040                               omgg thts so luckyyy       -1\n",
      "180515                          Is not feeling well again       -1\n",
      "675273   going to summer school on cruches and my compu...      -1\n",
      "500353                                           is tired       -1\n",
      "871672   going to my orientation and then baby sitting ...       1\n",
      "959683   lol that helps too but i have a sexy mcfly mug xx       1\n",
      "989230   its near twister everone is moving but the ren...       1\n",
      "1037868                  hello thank you for following me        1\n",
      "491529   these people are driving me crazy Im a human b...      -1\n",
      "444810   Why isn t there a drink that can knock me in a...      -1\n",
      "759146   http twitpic com 86z9u me and my cousin gavin ...      -1\n",
      "450296   hey I thought I was the cantankerous one not f...      -1\n",
      "938878   there is thin line between monopoly and busine...       1\n",
      "754388   i don t want jon and kate to get a divorce i h...      -1\n",
      "224503   I knew the Cavs were gonna lose going into the...      -1\n",
      "188892                                 Not tired at allll       -1\n",
      "852532   ok I gotta go back to getting my ass pwned on ...       1\n",
      "568650   I m coming in at 2 That s probably when you re...      -1\n",
      "468403   ready for work got to love radio only on the net       -1\n",
      "246766   i love sundays headin to pool with spendin som...      -1\n",
      "501331   Ugh I hate having tuitions in the morning Its ...      -1\n",
      "888641         What Why don t you give the shirt to a fan        1\n",
      "964333   email our team at We_Can_Help cable comcast co...       1\n",
      "758546   I lost by one year while playing quot remember...      -1\n",
      "259643   Cant sleep want ice cream have to go back to c...      -1\n",
      "...                                                    ...     ...\n",
      "500529       Aweee hell You musta done a job on your knee       -1\n",
      "721967   Checked the USGS and we didn t have an earthqu...      -1\n",
      "146341    haha i am soo glad you didn t say bananas man...      -1\n",
      "966056   congratulations and all the best for you and H...       1\n",
      "845048    which is just what T rex was thinking 200 mil...       1\n",
      "850995   happy birthday to me gettin ready for work and...       1\n",
      "192343   im in the room with my sis and being bored and...      -1\n",
      "726578   I hope so I just talked with Neal I can only h...      -1\n",
      "417618   if I get the hiccups I ll seriously have them ...      -1\n",
      "928729        Going out for a bite to EAT Back in a FLASH        1\n",
      "849225   is having dinner at cafe cartel marina to cele...       1\n",
      "843344   April 20 quot has long been an unofficial day ...       1\n",
      "1038401  MEAN MAN SO MEAN JK Have to pick on U alittle ...       1\n",
      "44268                           I think he is ignoring me       -1\n",
      "571446                         That s not very reassuring       -1\n",
      "972391   thx I know the over abundance can be annoying ...       1\n",
      "532988   i m going to college after this summer and i l...      -1\n",
      "528885   Now what No more Lakers BBQ Parties The World ...      -1\n",
      "82954    Gah Comp is acting up Dreadfully this means my...      -1\n",
      "584982   Thank you I hate that it looks different on di...      -1\n",
      "811608   today s renewed wedding invitation http www et...       1\n",
      "658680   It is I just worry that the computer will cras...      -1\n",
      "131267   The seniors class song is We All Roll Along an...      -1\n",
      "542163   Thanks for inviting me I thought her birthday ...      -1\n",
      "877443                                     loves shopping        1\n",
      "881862   just woken up feel amazing that i havent got w...       1\n",
      "624014   Going batty but of course I m a nut Breaker ke...      -1\n",
      "207623   doesn t want her caroline edwards original tat...      -1\n",
      "376763             1 2 the frog nicely attatched i miss S       -1\n",
      "240527   I m in the doghouse as i chucked OUR pool in t...      -1\n",
      "\n",
      "[1500 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "df3.loc[df3['Target'] > 3, 'Target'] = 1\n",
    "df3.loc[df3['Target'] < 1, 'Target'] = -1\n",
    "df3.loc[df3['Target'] == 2, 'Target'] = 0\n",
    "\n",
    "print(df3)\n",
    "\n",
    "#data has been cleaned and formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making train and test data\n",
    "#x = independant which in this case is the column 'text'\n",
    "#y = depedant which in this case is the column 'Target'\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df3['text']\n",
    "y = df3['Target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size = 0.20, random_state=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, recall_score, precision_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):     \n",
    "    tknzr = TweetTokenizer()\n",
    "    return tknzr.tokenize(text)\n",
    "\n",
    "def stem(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "en_stopwords = set(line.strip() for line in open(\"stopwords.txt\")) \n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer = tokenize,\n",
    "    lowercase = True,\n",
    "    ngram_range=(1, 1),\n",
    "    stop_words = en_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=5, random_state=30, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "kfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=30)\n",
    "print(kfolds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:   17.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7350701800847458"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "pipeline_svm = make_pipeline(vectorizer, \n",
    "                            SVC(probability=True, kernel=\"linear\", class_weight=\"balanced\"))\n",
    "\n",
    "grid_svm = GridSearchCV(pipeline_svm,\n",
    "                    param_grid = {'svc__C': [0.01, 0.1, 1, 10]}, \n",
    "                    cv = kfolds,\n",
    "                    scoring=\"roc_auc\",\n",
    "                    verbose=1,   \n",
    "                    n_jobs=-1) \n",
    "\n",
    "grid_svm.fit(X_train, y_train)\n",
    "grid_svm.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'svc__C': 0.1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_svm.best_params_\n",
    "#best c to use for SVM tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_svm.predict([\"ew this sucks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
